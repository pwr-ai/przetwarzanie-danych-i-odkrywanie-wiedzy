{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "greenhouse-interim",
   "metadata": {},
   "source": [
    "# Zadania Lista 4\n",
    "\n",
    "W zadaniu wykorzystamy zbiór danych [Reddit WallStreetBets Posts and Comments](https://www.kaggle.com/mattpodolak/rwallstreetbets-posts-and-comments?select=wallstreetbets_posts.csv), udostępniony na portalu Kaggle.\n",
    "![DS_LOGO](https://upload.wikimedia.org/wikipedia/en/f/f0/WallStreetBets.png)\n",
    "\n",
    "Zbiór danych zawiera posty i komentarze pochodzące z subreddita `wallstreetbets` z okresu 06-12-2020 do 06-02-2021 zebrane za pomocą narzędzia [pmaw](https://github.com/mattpodolak/pmaw/). W zadaniu wykorzystamy wyłącznie plik `wallstreetbets_posts.csv`. \n",
    "Lista zadań została podzielona na cztery etapy:\n",
    "1. Czyszczenie danych i przetwarzanie wstępne\n",
    "2. Inżynieria cech\n",
    "3. Przygotowanie modelu\n",
    "4. Selekcja cech\n",
    "\n",
    "Celem zadania jest przygotowanie modelu klasyfikacji, mogącego rozpoznać czy dany post zostanie usunięty przez moderatora."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "apparent-bundle",
   "metadata": {},
   "source": [
    "## 1. (4 pkt)  Czyszczenie danych i przetwarzanie wstępne \n",
    "a) **(0.5 pkt)** Usuń posty użytkowników ze zbioru danych, którzy napisali mniej niż 5 postów, usuń również użytkownika z maksymalną liczbą postów (bot). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "banner-circular",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TUTAJ UMIEŚĆ KOD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "existing-november",
   "metadata": {},
   "source": [
    "b) **(0.5 pkt)** Usuń zduplikowane wiersze.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "capable-lighting",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TUTAJ UMIEŚĆ KOD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "altered-mainland",
   "metadata": {},
   "source": [
    "c) **(0.5 pkt)** Stwórz kolumnę `target`, która będzie określać zbiór etykiet wyjściowych. W tym celu przetwórz kolumnę `removed_by_category`. Dla wartości kolumny `moderator, reddit, automod_filtered` przyjmij etykietę 1, w pozostałych przypadkach etykietę 0. Na koniec usuń kolumnę `removed_by_category` \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "encouraging-cancer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TUTAJ UMIEŚĆ KOD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "foster-report",
   "metadata": {},
   "source": [
    "d) **(2 pkt)** Wybierz i przedstaw podzbiór kolumn do dalszego przetwarzania. W tym celu możesz wykorzystać poznaną wcześniej bibliotekę `pandas_profiling`. Dodaj krótkie uzasadnienie do każdej z kolumn. Uzupełnij brakujące wartości, jeżeli zajdzie taka potrzeba.\n",
    "\n",
    "**Zbiór kolumn potrzebnych do dalszego przetwarzania. W ich przypadku, proszę nie pisać uzasadniania.**\n",
    "- author\n",
    "- title\n",
    "- created_utc\n",
    "\n",
    "**Kolumny, które muszą byc usunięte**\n",
    "- selftext\n",
    "- no_follow\n",
    "- crosspost_parent_list\n",
    "- is_robot_indexable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "waiting-convert",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TUTAJ UMIEŚĆ KOD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minus-afghanistan",
   "metadata": {},
   "source": [
    "\n",
    "**(0.5 pkt)** e) Dokonaj podziału zbioru na dwie części: uczący i testowy. Nie używaj podziału losowego, tylko podziel dane wg czasu napisania posta. Zbiór testowy mają tworzyć wpisy z ostatnich 6 dni, a wcześniejsze wpisy zbiór uczący  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "endless-unknown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TUTAJ UMIEŚĆ KOD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tested-allocation",
   "metadata": {},
   "source": [
    "##  2. (5 pkt) Inżynieria cech\n",
    "\n",
    "a) **(2 pkt)** Zaproponuj zestaw co najmniej 5 cech użytkownika zagregowanych do dnia napisania danego postu np. liczba wszystkich napisanych postów. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "radio-yahoo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TUTAJ UMIEŚĆ KOD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparative-ready",
   "metadata": {},
   "source": [
    "b) **(1 pkt)** Utwórz nowy zestaw cech na podstawie cech zaproponowych w poprzednim punkcie ograniczając je czasowo do ostatnich 7 dni.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "understanding-competition",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TUTAJ UMIEŚĆ KOD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "taken-taylor",
   "metadata": {},
   "source": [
    "c) **(2 pkt)** Przekształć cechę tekstową `title` wykorzystująć do tego metodę `TF-IDF `\n",
    "- Sprowadź tokeny do formy bazowej (lematy)\n",
    "- Odfiltruj najczęstsze słowa w języku angielskim (stop words) w tym celu możesz wykorzystać bibliotekę `spacy`. \n",
    "- Usuń znaki interpunkcyjne \n",
    "- Ogranicz reprezentację tf-idf do 25 najczęstszych słów w zbiorze \n",
    "- Dodaj do zbioru tak przygotowane cechy, zmapuj wektor cech do n kolumn np. `title_tfidf_gme`, `title_tfidf_squeeze` itd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abandoned-archive",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TUTAJ UMIEŚĆ KOD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blocked-lawyer",
   "metadata": {},
   "source": [
    "\n",
    "## 3. (4 pkt) Przygotowanie modelu\n",
    "\n",
    "a) **(2 pkt)** Wyucz klasyfikator XBGClassifier z modułu (xgboost). Wykorzystaj wersję biblioteki 1.3.3\n",
    "Przeprowadź odpowiednie transformacje danych, wymagane dla tego modelu. Do ewaluacji wykorzystaj metryki `Precision`,  `Recall` i `F1` oraz zareportuj czas uczenia. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lesbian-prague",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TUTAJ UMIEŚĆ KOD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pressed-dietary",
   "metadata": {},
   "source": [
    "\n",
    "b) **(2 pkt)** Zmień reprezentację tekstową pola `title`. Przeprowadź podobne kroki jak w punkcie `2 c)`, tylko zamiast ograniczania reprezentacji do 25 najczęstszych słów w zbiorze, wykorzystaj metodę `PCA`. Zareportuj pokrycie wariancji. Tak otrzymane cechy z `pca` dodaj do zbioru uczącego. Przeucz model i dokonaj porównania. Krótko podsumuj wyniki."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interior-electronics",
   "metadata": {},
   "source": [
    "## 4. (2 pkt) Selekcja cech \n",
    "\n",
    "Dokonaj selekcji cech modelu. Do dalszego przetwarzania wybierz model z punktu `3 c)` oferujący lepszą wartość metryki F1. Do wybrania odpowiedniego podzbioru cech wykorzystaj bibliotekę `shap`. Dokonaj przeuczenia modelu. Porównaj wyniki i czas obliczeń do modelu wykorzystującego cały zbiór cech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tight-logic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TUTAJ UMIEŚĆ KOD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "auburn-direction",
   "metadata": {},
   "source": [
    "## Dodatkowe zadanie (+1 pkt)\n",
    "\n",
    "Wykorzystaj bibliotekę dedykowaną do automatycznej ekstrakcji cech np. `featuretoools`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
